{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d57e095",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/smurugan9/research/aislens/AISLENS/src/aislens/utils.py:5: UserWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas will still use PyGEOS by default for now. To force to use and test Shapely 2.0, you have to set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In a future release, GeoPandas will switch to using Shapely by default. If you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "# This script calculates comprehensive draft dependence parameters using changepoint detection\n",
    "# and piecewise linear models for Antarctic ice shelves.\n",
    "# \n",
    "# This enhanced version creates all 5 draft dependence parameters:\n",
    "#   - draftDepenBasalMelt_minDraft: threshold draft value (0 for noisy shelves)\n",
    "#   - draftDepenBasalMelt_constantMeltValue: constant melt rate for shallow areas  \n",
    "#   - draftDepenBasalMelt_paramType: selector (0 for linear, 1 for constant)\n",
    "#   - draftDepenBasalMeltAlpha0: intercept (0 for noisy shelves)\n",
    "#   - draftDepenBasalMeltAlpha1: slope (0 for noisy shelves)\n",
    "#\n",
    "# Run this script after running prepare_data.py\n",
    "\n",
    "from aislens.config import config\n",
    "from aislens.utils import write_crs, merge_catchment_data\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b775733a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ruptures library available (version: 1.1.9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check for optional dependencies\n",
    "try:\n",
    "    import ruptures\n",
    "    RUPTURES_AVAILABLE = True\n",
    "    print(f\"✓ ruptures library available (version: {ruptures.__version__})\")\n",
    "except ImportError:\n",
    "    RUPTURES_AVAILABLE = False\n",
    "    print(\" WARNING: ruptures library not available - changepoint detection will fail!\")\n",
    "    print(\"   Install with: pip install ruptures\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    PANDAS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PANDAS_AVAILABLE = False\n",
    "    print(\" WARNING: pandas library not available - summary creation will fail!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891de4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_draft_dependence_comprehensive(icems, satobs, config,\n",
    "                                           n_bins=50, min_points_per_bin=5,\n",
    "                                           ruptures_method='pelt', ruptures_penalty=1.0,\n",
    "                                           min_r2_threshold=0.05, min_correlation=0.1,\n",
    "                                           noisy_fallback='zero', model_selection='best'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive draft dependence parameters for all ice shelf regions.\n",
    "\n",
    "    Args:\n",
    "        icems: GeoDataFrame with ice shelf masks\n",
    "        satobs: xarray Dataset with satellite observations\n",
    "        config: Configuration object\n",
    "        n_bins: Number of bins for draft binning (default: 50)\n",
    "        min_points_per_bin: Minimum points required per bin (default: 5)\n",
    "        ruptures_method: Ruptures method ('pelt', 'binseg', 'window') (default: 'pelt')\n",
    "        ruptures_penalty: Penalty parameter for ruptures (default: 1.0)\n",
    "        min_r2_threshold: Minimum R² for meaningful relationship (default: 0.1)\n",
    "        min_correlation: Minimum correlation for meaningful relationship (default: 0.3)\n",
    "        noisy_fallback: For noisy data ('zero' or 'mean') (default: 'zero')\n",
    "        model_selection: Which model to use ('best', 'zero_shallow', 'mean_shallow', 'threshold_intercept') (default: 'best')\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"CALCULATING COMPREHENSIVE DRAFT DEPENDENCE PARAMETERS...\")\n",
    "    print(f\"Settings: method={ruptures_method}, penalty={ruptures_penalty}\")\n",
    "    print(f\"Quality thresholds: R²≥{min_r2_threshold}, |corr|≥{min_correlation}\")\n",
    "    print(f\"Model selection: {model_selection}, noisy fallback: {noisy_fallback}\")\n",
    "    print(f\"Processing ice shelves starting from index 33 (Abbott Ice Shelf)\")\n",
    "\n",
    "    # Debug information\n",
    "    print(f\"Total ice shelves to process: {len(list(icems.name.values[33:]))}\")  # Count from index 33 onwards\n",
    "    print(f\"icems dataframe length: {len(icems)}\")\n",
    "    print(f\"Satellite data shape: {satobs[config.SATOBS_FLUX_VAR].shape}\")\n",
    "\n",
    "    # Check ice shelf names for first few\n",
    "    print(\"Sample ice shelf names (starting from index 33):\")\n",
    "    for i, name in enumerate(list(icems.name.values[33:40])):  # Show first 7 from index 33\n",
    "        actual_index = i + 33\n",
    "        print(f\"  Index {actual_index}: {name}\")\n",
    "\n",
    "    if len(icems) <= 40:\n",
    "        print(f\"⚠️  WARNING: icems only has {len(icems)} rows, so only {max(0, len(icems) - 33)} ice shelves will be processed\")\n",
    "    else:\n",
    "        expected_count = len(icems) - 33\n",
    "        print(f\"Expected to process ~{expected_count} ice shelves (from index 33 to {len(icems)-1})\")\n",
    "\n",
    "    # Create save directory for comprehensive results\n",
    "    save_dir_comprehensive = config.DIR_ICESHELF_DEDRAFT_SATOBS / \"comprehensive\"\n",
    "    save_dir_comprehensive.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Store results for each ice shelf\n",
    "    all_results = {}\n",
    "    all_draft_params = {}\n",
    "\n",
    "    # Process each ice shelf - use sequential processing like notebook\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    error_details = {}  # Track specific error types\n",
    "\n",
    "    # Get ice shelf names starting from index 33 (like notebook does with [33:])\n",
    "    shelf_names = list(icems.name.values[33:])  # Start from Abbott Ice Shelf\n",
    "\n",
    "    for i, shelf_name in enumerate(shelf_names):\n",
    "        actual_index = i + 33  # Convert back to actual DataFrame index\n",
    "\n",
    "        print(f\"  Starting comprehensive analysis...\")\n",
    "        result = dedraft_catchment_comprehensive(\n",
    "            actual_index, icems, satobs, config,  # Use actual_index instead of i\n",
    "            save_dir=save_dir_comprehensive,\n",
    "            weights=None,  # Don't use weights - set to None instead of False\n",
    "            weight_power=0.25,\n",
    "            save_pred=True,\n",
    "            save_coefs=True,\n",
    "            n_bins=n_bins,\n",
    "            min_points_per_bin=min_points_per_bin,\n",
    "            ruptures_method=ruptures_method,\n",
    "            ruptures_penalty=ruptures_penalty,\n",
    "            min_r2_threshold=min_r2_threshold,\n",
    "            min_correlation=min_correlation,\n",
    "            noisy_fallback=noisy_fallback,\n",
    "            model_selection=model_selection\n",
    "        )\n",
    "\n",
    "        def calculate_draft_threshold_and_predict_ruptures_comprehensive(obs23_melt_tm, obs23_draft_tm, icems, clip_data, \n",
    "                                                               n_bins=50, min_points_per_bin=5, \n",
    "                                                               ruptures_method='pelt', ruptures_penalty=1.0,\n",
    "                                                               plot_individual=True, save_plots=False, save_dir=None,\n",
    "                                                               min_r2_threshold=0.1, min_correlation=0.3,\n",
    "                                                               noisy_fallback='zero'):  # 'zero' or 'mean'\n",
    "\n",
    "        print(f\"  Analysis completed successfully!\")\n",
    "\n",
    "        all_results[shelf_name] = result['full_results']\n",
    "        all_draft_params[shelf_name] = result['draft_params']\n",
    "        processed_count += 1\n",
    "\n",
    "        print(f\"✓ Processed {shelf_name}: \"\n",
    "                f\"meaningful={result['full_results']['is_meaningful']}, \"\n",
    "                f\"paramType={result['draft_params']['paramType']}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nProcessing Summary:\")\n",
    "    print(f\"  Successfully processed: {processed_count} ice shelves\")\n",
    "    print(f\"  Expected total: {len(shelf_names)} ice shelves\")\n",
    "\n",
    "    if processed_count == 0:\n",
    "        print(f\"\\n⚠️  WARNING: No ice shelves were processed successfully!\")\n",
    "        print(f\"   This suggests a systematic issue. Common causes:\")\n",
    "        print(f\"   1. Missing dependencies (ruptures library)\")\n",
    "        print(f\"   2. Data format/coordinate issues\")\n",
    "        print(f\"   3. Index range problems\")\n",
    "        print(f\"   4. Insufficient data in ice shelf regions\")\n",
    "        return {}, {}  # Return empty results to avoid downstream errors\n",
    "\n",
    "    # Create comprehensive summary\n",
    "    create_comprehensive_summary(all_results, all_draft_params, save_dir_comprehensive)\n",
    "\n",
    "    # Merge parameters into ice sheet grids\n",
    "    merge_comprehensive_parameters(all_draft_params, icems, satobs, config, save_dir_comprehensive)\n",
    "\n",
    "    print(\"COMPREHENSIVE DRAFT DEPENDENCE PARAMETERS CALCULATED AND SAVED.\")\n",
    "\n",
    "    return all_results, all_draft_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b499652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_summary(all_results, all_draft_params, save_dir):\n",
    "    \"\"\"Create summary statistics and save to CSV.\"\"\"\n",
    "\n",
    "    summary_data = []\n",
    "\n",
    "    for shelf_name, result in all_results.items():\n",
    "        draft_params = all_draft_params[shelf_name]\n",
    "\n",
    "        summary_data.append({\n",
    "            'shelf_name': shelf_name,\n",
    "            'is_meaningful': result['is_meaningful'],\n",
    "            'correlation': result.get('correlation', np.nan),\n",
    "            'r2': result.get('r2', np.nan),\n",
    "            'threshold_draft': result.get('threshold', np.nan),\n",
    "            'slope': result.get('slope', 0.0),\n",
    "            'shallow_mean': result.get('shallow_mean', 0.0),\n",
    "            'n_points': len(result.get('melt_vals', [])),\n",
    "            'minDraft': draft_params['minDraft'],\n",
    "            'constantValue': draft_params['constantValue'],\n",
    "            'paramType': draft_params['paramType'],\n",
    "            'alpha0': draft_params['alpha0'],\n",
    "            'alpha1': draft_params['alpha1']\n",
    "        })\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    # Save summary\n",
    "    summary_file = save_dir / \"comprehensive_summary.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"Summary saved to {summary_file}\")\n",
    "\n",
    "    # Print key statistics\n",
    "    meaningful_count = summary_df['is_meaningful'].sum()\n",
    "    total_count = len(summary_df)\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    print(f\"  Total shelves processed: {total_count}\")\n",
    "    print(f\"  Meaningful relationships: {meaningful_count} ({meaningful_count/total_count*100:.1f}%)\")\n",
    "    print(f\"  Linear parameterization (paramType=0): {(summary_df['paramType']==0).sum()}\")\n",
    "    print(f\"  Constant parameterization (paramType=1): {(summary_df['paramType']==1).sum()}\")\n",
    "    print(f\"  Mean correlation (meaningful only): {summary_df[summary_df['is_meaningful']]['correlation'].mean():.3f}\")\n",
    "    print(f\"  Mean R² (meaningful only): {summary_df[summary_df['is_meaningful']]['r2'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_comprehensive_parameters(all_draft_params, icems, satobs, config, save_dir):\n",
    "    \"\"\"Merge individual ice shelf parameters into full ice sheet grids.\"\"\"\n",
    "\n",
    "    # Get reference spatial grid from satellite observations\n",
    "    ref_grid = satobs[config.SATOBS_FLUX_VAR].isel({config.TIME_DIM: 0}) if config.TIME_DIM in satobs[config.SATOBS_FLUX_VAR].dims else satobs[config.SATOBS_FLUX_VAR]\n",
    "\n",
    "    # Initialize empty datasets for each parameter with full spatial grid\n",
    "    config_param_names = [\n",
    "        'draftDepenBasalMelt_minDraft',\n",
    "        'draftDepenBasalMelt_constantMeltValue',\n",
    "        'draftDepenBasalMelt_paramType',\n",
    "        'draftDepenBasalMeltAlpha0',\n",
    "        'draftDepenBasalMeltAlpha1'\n",
    "    ]\n",
    "\n",
    "    # Create full-grid datasets initialized with zeros/NaN\n",
    "    merged_datasets = {}\n",
    "    for config_param_name in config_param_names:\n",
    "        # Initialize with zeros to match original behavior\n",
    "        full_grid = xr.zeros_like(ref_grid)\n",
    "        full_grid.name = config_param_name\n",
    "        full_grid.attrs = config.DATA_ATTRS[config_param_name]\n",
    "        merged_datasets[config_param_name] = xr.Dataset({config_param_name: full_grid})\n",
    "        print(f\"Initialized {config_param_name} with shape: {full_grid.shape}\")\n",
    "\n",
    "    # Merge individual catchment files onto the full grid\n",
    "    merged_count = 0\n",
    "    for shelf_name in all_draft_params.keys():\n",
    "        for config_param_name in config_param_names:\n",
    "            try:\n",
    "                # Look for individual parameter files\n",
    "                param_file = save_dir / f\"{config_param_name}_{shelf_name}.nc\"\n",
    "                if param_file.exists():\n",
    "                    # Load the individual ice shelf parameter file\n",
    "                    param_ds = xr.open_dataset(param_file)\n",
    "                    param_da = param_ds[config_param_name]\n",
    "\n",
    "                    # Check if the coordinates match\n",
    "                    if not (param_da.x.equals(ref_grid.x) and param_da.y.equals(ref_grid.y)):\n",
    "                        print(f\"Warning: Coordinate mismatch for {shelf_name} {config_param_name}\")\n",
    "                        print(f\"  Ice shelf shape: {param_da.shape}, Full grid shape: {ref_grid.shape}\")\n",
    "                        print(f\"  Ice shelf x range: [{param_da.x.min().values:.1f}, {param_da.x.max().values:.1f}]\")\n",
    "                        print(f\"  Ice shelf y range: [{param_da.y.min().values:.1f}, {param_da.y.max().values:.1f}]\")\n",
    "                        print(f\"  Full grid x range: [{ref_grid.x.min().values:.1f}, {ref_grid.x.max().values:.1f}]\")\n",
    "                        print(f\"  Full grid y range: [{ref_grid.y.min().values:.1f}, {ref_grid.y.max().values:.1f}]\")\n",
    "\n",
    "                        # Try to align the data by interpolating to the full grid coordinates\n",
    "                        try:\n",
    "                            # First, ensure we have proper coordinate alignment\n",
    "                            param_da_aligned = param_da.interp(\n",
    "                                x=ref_grid.x,\n",
    "                                y=ref_grid.y,\n",
    "                                method='nearest',\n",
    "                                fill_value=0  # Fill outside interpolation range with 0\n",
    "                            )\n",
    "\n",
    "                            # Ensure the aligned data has the same shape as ref_grid\n",
    "                            if param_da_aligned.shape != ref_grid.shape:\n",
    "                                print(f\"  Shape mismatch after interpolation: {param_da_aligned.shape} vs {ref_grid.shape}\")\n",
    "                                continue\n",
    "\n",
    "                            # Create a mask for non-zero values (ice shelf regions)\n",
    "                            valid_mask = (param_da_aligned != 0) & (~param_da_aligned.isnull())\n",
    "\n",
    "                            if valid_mask.any():\n",
    "                                # Get the current merged grid\n",
    "                                current_grid = merged_datasets[config_param_name][config_param_name]\n",
    "\n",
    "                                # Ensure shapes match before merging\n",
    "                                if current_grid.shape != param_da_aligned.shape:\n",
    "                                    print(f\"  Grid shape mismatch: {current_grid.shape} vs {param_da_aligned.shape}\")\n",
    "                                    continue\n",
    "\n",
    "                                # Update only where we have valid ice shelf data\n",
    "                                updated_grid = current_grid.where(~valid_mask, param_da_aligned)\n",
    "                                merged_datasets[config_param_name][config_param_name] = updated_grid\n",
    "\n",
    "                                merged_count += 1\n",
    "                                valid_points = valid_mask.sum().values\n",
    "                                print(f\"  Successfully interpolated and merged {shelf_name} {config_param_name} ({valid_points} points)\")\n",
    "                            else:\n",
    "                                print(f\"  No valid data after interpolation for {shelf_name} {config_param_name}\")\n",
    "\n",
    "                        except Exception as interp_error:\n",
    "                            print(f\"  Failed to interpolate {shelf_name} {config_param_name}: {interp_error}\")\n",
    "                            traceback.print_exc()\n",
    "                            continue\n",
    "                    else:\n",
    "                        # Coordinates match, can directly merge\n",
    "                        try:\n",
    "                            # Use non-null and non-zero values as the mask\n",
    "                            overlap_mask = (~param_da.isnull()) & (param_da != 0)\n",
    "\n",
    "                            if overlap_mask.any():\n",
    "                                # Get current grid and ensure shapes match\n",
    "                                current_grid = merged_datasets[config_param_name][config_param_name]\n",
    "\n",
    "                                if current_grid.shape != param_da.shape:\n",
    "                                    print(f\"  Direct merge shape mismatch: {current_grid.shape} vs {param_da.shape}\")\n",
    "                                    continue\n",
    "\n",
    "                                # Update grid where ice shelf data exists\n",
    "                                updated_grid = current_grid.where(~overlap_mask, param_da)\n",
    "                                merged_datasets[config_param_name][config_param_name] = updated_grid\n",
    "\n",
    "                                merged_count += 1\n",
    "                                valid_points = overlap_mask.sum().values\n",
    "                                print(f\"  Successfully merged {shelf_name} {config_param_name} ({valid_points} points)\")\n",
    "                            else:\n",
    "                                print(f\"  No valid data for direct merge of {shelf_name} {config_param_name}\")\n",
    "\n",
    "                        except Exception as merge_error:\n",
    "                            print(f\"  Failed to directly merge {shelf_name} {config_param_name}: {merge_error}\")\n",
    "                            traceback.print_exc()\n",
    "                            continue\n",
    "\n",
    "                else:\n",
    "                    print(f\"Warning: File not found: {param_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not merge {config_param_name} for {shelf_name}: {e}\")\n",
    "                traceback.print_exc()\n",
    "\n",
    "    print(f\"Successfully merged {merged_count} parameter files onto full grids\")\n",
    "\n",
    "    # Save merged datasets\n",
    "    for config_param_name, merged_ds in merged_datasets.items():\n",
    "        # Save individual parameter file\n",
    "        output_file = config.DIR_PROCESSED / f\"draft_dependence_changepoint\" / f\"ruptures_{config_param_name}.nc\"\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        merged_ds.to_netcdf(output_file)\n",
    "        print(f\"Saved {config_param_name} to {output_file} with shape: {merged_ds[config_param_name].shape}\")\n",
    "\n",
    "    # Create combined dataset with all parameters\n",
    "    combined_ds = xr.Dataset()\n",
    "    for config_param_name, merged_ds in merged_datasets.items():\n",
    "        combined_ds = xr.merge([combined_ds, merged_ds])\n",
    "\n",
    "    # Save combined file\n",
    "    combined_file = config.DIR_PROCESSED / \"draft_dependence_changepoint\" / \"ruptures_draftDepenBasalMelt_parameters.nc\"\n",
    "    combined_ds.to_netcdf(combined_file)\n",
    "    print(f\"Saved combined parameters to {combined_file}\")\n",
    "    print(f\"Combined dataset shape: {list(combined_ds.dims.values())}\")\n",
    "    print(f\"Combined dataset variables: {list(combined_ds.data_vars.keys())}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    print(\"Loading satellite observation data...\")\n",
    "    satobs = xr.open_dataset(config.FILE_PAOLO23_SATOBS_PREPARED)\n",
    "    satobs = write_crs(satobs, config.CRS_TARGET)\n",
    "\n",
    "    print(\"Loading ice shelf masks...\")\n",
    "    icems = gpd.read_file(config.FILE_ICESHELFMASKS)\n",
    "    icems = icems.to_crs({'init': config.CRS_TARGET})\n",
    "\n",
    "    # Run comprehensive analysis\n",
    "    # Note: Now processes ice shelves sequentially starting from index 33 (Abbott Ice Shelf)\n",
    "    all_results, all_draft_params = calculate_draft_dependence_comprehensive(\n",
    "        icems, satobs, config,\n",
    "        n_bins=50,\n",
    "        min_points_per_bin=5,\n",
    "        ruptures_method='pelt',\n",
    "        ruptures_penalty=1.0,\n",
    "        min_r2_threshold=0.1,\n",
    "        min_correlation=0.2,\n",
    "        noisy_fallback='zero',\n",
    "        model_selection='best'\n",
    "    )\n",
    "\n",
    "    print(f\"\\nProcessing complete! Processed {len(all_results)} ice shelves.\")\n",
    "    print(\"Output files saved to:\")\n",
    "    print(f\"  - Individual files: {config.DIR_ICESHELF_DEDRAFT_SATOBS / 'comprehensive'}\")\n",
    "    print(f\"  - Merged grids: {config.DIR_PROCESSED / 'draft_dependence_changepoint'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aislens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
